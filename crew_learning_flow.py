"""
Complete Multi-Agent Flow with Online Learning Demonstration
=============================================================

This shows:
1. Complete 7-step flow per timestep
2. Episode-by-episode improvement
3. Before/After comparisons
4. CrewAI analyzing learning progress

Perfect for presentations!
"""

import numpy as np
from typing import Dict, List
from dataclasses import dataclass
from crewai import Agent, Task, Crew, Process
import time


class CompleteFlowDemo:
    """Demonstrates complete flow with visible online learning"""
    
    def __init__(self):
        from src.agent_utils import (EnhancedControllerAgent, EnhancedBiddingAgent,
                          EnhancedBudgetAgent, EnhancedSimulationAgent, Action)
        
        self.controller = EnhancedControllerAgent(state_dim=10)
        self.bidding_agent = EnhancedBiddingAgent(state_dim=10)
        self.budget_agent = EnhancedBudgetAgent(state_dim=10)
        self.env = EnhancedSimulationAgent()
        self.Action = Action
        
        self.episode_history = []
        
        # CrewAI analyst
        self.analyst = Agent(
            role='Online Learning Analyst & System Monitor',
            goal='Analyze and explain how multi-agent RL system improves through experience',
            backstory="""Expert in reinforcement learning and multi-agent systems. 
            You excel at identifying learning patterns and explaining how agents 
            adapt their strategies based on experience.""",
            verbose=True,
            allow_delegation=False
        )
    
    def show_complete_flow_for_one_step(self, step_num: int, state, training: bool = True):
        """Show complete 7-step flow for ONE timestep"""
        
        print(f"\n{'â”€'*80}")
        print(f"TIMESTEP {step_num} - COMPLETE FLOW")
        print(f"{'â”€'*80}")
        
        state_array = state.to_array()
        
        # STEP 1: SimulationAgent â†’ ControllerAgent
        print(f"\n1ï¸âƒ£  SimulationAgent â†’ ControllerAgent")
        print(f"   ğŸ“¤ Sends current state")
        print(f"   State: Budget=${state.remaining_budget:.0f}, CPC=${state.current_cpc:.2f}, "
              f"Comp={state.competition_level:.2f}")
        time.sleep(0.3)
        
        # STEP 2: ControllerAgent queries specialists
        print(f"\n2ï¸âƒ£  ControllerAgent â†’ BiddingAgent & BudgetAgent")
        print(f"   ğŸ¤” Analyzing state and deciding which agents to query...")
        time.sleep(0.3)
        
        controller_action, _, entropy = self.controller.select_action(state_array, training=training)
        action_name = ["Bidding Only", "Budget Only", "Both"][controller_action]
        
        print(f"   âœ“ Decision: Query {action_name}")
        print(f"   Confidence: {1-entropy:.2f} (entropy={entropy:.3f})")
        
        use_bidding = controller_action in [0, 2]
        use_budget = controller_action in [1, 2]
        
        # STEP 3: BiddingAgent â†’ ControllerAgent
        print(f"\n3ï¸âƒ£  BiddingAgent â†’ ControllerAgent")
        if use_bidding:
            bid, bid_idx = self.bidding_agent.select_bid(state_array, training=training)
            print(f"   ğŸ’° Proposal: Bid ${bid:.2f}")
            print(f"   Exploration (Îµ): {self.bidding_agent.epsilon:.4f}")
            time.sleep(0.3)
        else:
            bid, bid_idx = 2.0, 5
            print(f"   â­ï¸  Not queried (using default ${bid:.2f})")
        
        # STEP 4: BudgetAgent â†’ ControllerAgent
        print(f"\n4ï¸âƒ£  BudgetAgent â†’ ControllerAgent")
        if use_budget:
            allocation = self.budget_agent.select_allocation(state_array, training=training)
            print(f"   ğŸ“Š Proposal: {', '.join([f'{k}={v:.1%}' for k, v in list(allocation.items())])}")
            time.sleep(0.3)
        else:
            allocation = {f"Channel_{i+1}": 1.0/3 for i in range(3)}
            print(f"   â­ï¸  Not queried (using equal split)")
        
        # STEP 5: ControllerAgent â†’ SimulationAgent
        print(f"\n5ï¸âƒ£  ControllerAgent â†’ SimulationAgent")
        action = self.Action(
            bid_amount=bid,
            budget_allocation=allocation,
            agent_type=["bidding", "budget", "both"][controller_action]
        )
        print(f"   ğŸ¯ Executes combined action: Bid=${bid:.2f}, Allocation={action.agent_type}")
        time.sleep(0.3)
        
        # STEP 6: SimulationAgent â†’ All RL Agents
        print(f"\n6ï¸âƒ£  SimulationAgent â†’ All RL Agents")
        print(f"   âš™ï¸  Processing auction...")
        time.sleep(0.3)
        
        next_state, reward, done, info = self.env.step(action)
        next_state_array = next_state.to_array()
        
        print(f"   ğŸ“¥ Returns: Reward={reward:+.2f}, Conversions={info['conversions']}, "
              f"Clicks={info['clicks']}, Cost=${info['cost']:.2f}")
        
        # Store experiences
        self.controller.store_reward(reward, done)
        if use_bidding:
            self.bidding_agent.store_experience(state_array, bid_idx, reward, 
                                               next_state_array, done)
        if use_budget:
            self.budget_agent.store_reward(reward, done)
        
        print(f"   ğŸ’¾ Experiences stored for learning")
        
        return next_state, reward, done, info
    
    def run_episode_with_flow(self, episode: int, show_all_steps: bool = False):
        """Run complete episode showing flow"""
        
        print(f"\n{'='*80}")
        print(f"EPISODE {episode} - MULTI-AGENT ORCHESTRATION")
        print(f"{'='*80}")
        
        state = self.env.reset()
        episode_reward = 0
        done = False
        step = 0
        
        bids = []
        conversions_per_step = []
        
        # Show first few steps in detail
        steps_to_show = 3 if not show_all_steps else 100
        
        while not done and step < steps_to_show:
            next_state, reward, done, info = self.show_complete_flow_for_one_step(
                step, state, training=True
            )
            
            episode_reward += reward
            bids.append(info.get('bid', 0))
            conversions_per_step.append(info['conversions'])
            
            state = next_state
            step += 1
        
        # Continue rest of episode quietly
        if not done:
            print(f"\n{'â”€'*80}")
            print(f"Steps {step}-{self.env.max_steps}: Continuing in fast mode...")
            print(f"{'â”€'*80}")
            
            while not done:
                state_array = state.to_array()
                controller_action, _, _ = self.controller.select_action(state_array, training=True)
                
                use_bidding = controller_action in [0, 2]
                use_budget = controller_action in [1, 2]
                
                if use_bidding:
                    bid, bid_idx = self.bidding_agent.select_bid(state_array, training=True)
                else:
                    bid, bid_idx = 2.0, 5
                
                if use_budget:
                    allocation = self.budget_agent.select_allocation(state_array, training=True)
                else:
                    allocation = {f"Channel_{i+1}": 1.0/3 for i in range(3)}
                
                action = self.Action(bid_amount=bid, budget_allocation=allocation,
                                   agent_type=["bidding", "budget", "both"][controller_action])
                
                next_state, reward, done, info = self.env.step(action)
                next_state_array = next_state.to_array()
                
                self.controller.store_reward(reward, done)
                if use_bidding:
                    self.bidding_agent.store_experience(state_array, bid_idx, reward,
                                                       next_state_array, done)
                if use_budget:
                    self.budget_agent.store_reward(reward, done)
                
                episode_reward += reward
                state = next_state
                step += 1
        
        # Episode summary
        roi = 0
        if self.env.total_spend > 0:
            roi = ((self.env.total_conversions * self.env.conversion_value - 
                   self.env.total_spend) / self.env.total_spend) * 100
        
        print(f"\n{'='*80}")
        print(f"EPISODE {episode} SUMMARY")
        print(f"{'='*80}")
        print(f"Total Steps: {step}")
        print(f"Total Reward: {episode_reward:.2f}")
        print(f"Conversions: {self.env.total_conversions}")
        print(f"Clicks: {self.env.total_clicks}")
        print(f"Spend: ${self.env.total_spend:.2f}")
        print(f"ROI: {roi:.1f}%")
        
        metrics = {
            'episode': episode,
            'reward': episode_reward,
            'conversions': self.env.total_conversions,
            'clicks': self.env.total_clicks,
            'spend': self.env.total_spend,
            'roi': roi,
            'epsilon': self.bidding_agent.epsilon
        }
        
        self.episode_history.append(metrics)
        
        return metrics
    
    def demonstrate_learning(self, num_episodes: int = 5):
        """Demonstrate complete flow with visible learning"""
        
        print(f"\n{'='*80}")
        print("MULTI-AGENT ONLINE LEARNING DEMONSTRATION")
        print(f"{'='*80}")
        print(f"\nDemonstrating {num_episodes} episodes:")
        print("  â€¢ Complete 7-step flow shown")
        print("  â€¢ Agents learn after each episode")
        print("  â€¢ Performance improves over time")
        print("  â€¢ CrewAI analyzes learning progress")
        print(f"{'='*80}\n")
        
        for episode in range(num_episodes):
            # Run episode with complete flow
            metrics = self.run_episode_with_flow(episode, show_all_steps=(episode == 0))
            
            # STEP 7: CrewAI Analytics
            print(f"\n{'='*80}")
            print(f"7ï¸âƒ£  ANALYTICS AGENT (CrewAI) â†’ Reads logs & generates insights")
            print(f"{'='*80}")
            
            self.run_learning_analysis(episode)
            
            # Show learning happening
            print(f"\n{'='*80}")
            print(f"ğŸ§  AGENTS LEARNING FROM EPISODE {episode}")
            print(f"{'='*80}")
            
            print("\nğŸ”„ Updating agents based on experience...")
            
            # Controller learns
            controller_metrics = self.controller.update(epochs=5)
            if controller_metrics:
                print(f"   âœ“ Controller updated: Loss={controller_metrics.get('policy_loss', 0):.4f}")
            
            # Bidding agent learns
            updates = 0
            for _ in range(4):
                if len(self.bidding_agent.replay_buffer) >= 32:
                    self.bidding_agent.update()
                    updates += 1
            
            if updates > 0:
                print(f"   âœ“ Bidding Agent updated: {updates} learning steps, "
                      f"Îµ={self.bidding_agent.epsilon:.4f}")
            
            # Budget agent learns
            budget_metrics = self.budget_agent.update(epochs=5)
            if budget_metrics:
                print(f"   âœ“ Budget Agent updated: Loss={budget_metrics.get('policy_loss', 0):.4f}")
            
            print("\n   ğŸ’¡ Agents have now learned from this experience!")
            print("      Next episode will use the improved policies.")
            
            # Show improvement comparison
            if episode > 0:
                self.show_learning_comparison(episode)
            
            # Pause between episodes
            if episode < num_episodes - 1:
                input(f"\nâ–¶ï¸  Press Enter to see Episode {episode + 1} with improved agents...")
        
        # Final summary
        self.show_final_learning_summary(num_episodes)
    
    def run_learning_analysis(self, episode: int):
        """Run CrewAI analysis on learning progress"""
        
        if len(self.episode_history) == 0:
            return
        
        current = self.episode_history[-1]
        
        # Build analysis prompt
        if len(self.episode_history) > 1:
            previous = self.episode_history[-2]
            improvement = current['reward'] - previous['reward']
            
            analysis_prompt = f"""
Analyze the online learning demonstrated in this episode:

CURRENT EPISODE ({episode}):
- Reward: {current['reward']:.2f}
- Conversions: {current['conversions']}
- ROI: {current['roi']:.1f}%
- Exploration (Îµ): {current['epsilon']:.4f}

PREVIOUS EPISODE ({episode-1}):
- Reward: {previous['reward']:.2f}
- Conversions: {previous['conversions']}
- ROI: {previous['roi']:.1f}%

IMPROVEMENT:
- Reward Change: {improvement:+.2f} ({(improvement/max(abs(previous['reward']), 1)*100):+.1f}%)
- Conversion Change: {current['conversions'] - previous['conversions']:+d}
- ROI Change: {current['roi'] - previous['roi']:+.1f}pp

The agents just updated their policies based on Episode {episode-1} experience.

Explain:
1. What evidence shows the agents are learning?
2. What specific improvements occurred?
3. What strategy changes are visible?
4. Is this consistent with online learning theory?

Be specific and reference the numbers.
            """
        else:
            analysis_prompt = f"""
This is the baseline episode (Episode 0):

Performance:
- Reward: {current['reward']:.2f}
- Conversions: {current['conversions']}
- ROI: {current['roi']:.1f}%

The agents will now learn from this experience.
Explain what you expect to improve in the next episode.
            """
        
        task = Task(
            description=analysis_prompt,
            expected_output="Analysis of learning progress with specific evidence",
            agent=self.analyst
        )
        
        crew = Crew(
            agents=[self.analyst],
            tasks=[task],
            process=Process.sequential,
            verbose=False
        )
        
        try:
            print("\nğŸ¤– CrewAI Learning Analyst analyzing...\n")
            result = crew.kickoff()
            print("â”€" * 80)
            print(result)
            print("â”€" * 80)
        except Exception as e:
            print(f"âš ï¸  CrewAI analysis unavailable: {e}")
            # Fallback to rule-based analysis
            if len(self.episode_history) > 1:
                improvement = current['reward'] - previous['reward']
                if improvement > 0:
                    print("âœ“ EVIDENCE OF LEARNING: Reward increased after agent updates")
                    print(f"  The agents learned better policies from Episode {episode-1}")
                else:
                    print("â€¢ Exploring new strategies (temporary dip is normal in RL)")
    
    def show_learning_comparison(self, episode: int):
        """Show before/after comparison"""
        
        if episode < 1:
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š LEARNING COMPARISON: Episode {episode-1} â†’ Episode {episode}")
        print(f"{'='*80}\n")
        
        prev = self.episode_history[-2]
        curr = self.episode_history[-1]
        
        # Create comparison table
        metrics = [
            ('Reward', prev['reward'], curr['reward'], lambda x: f"{x:.2f}"),
            ('Conversions', prev['conversions'], curr['conversions'], lambda x: f"{x}"),
            ('ROI', prev['roi'], curr['roi'], lambda x: f"{x:.1f}%"),
            ('Spend', prev['spend'], curr['spend'], lambda x: f"${x:.2f}"),
            ('Epsilon (Îµ)', prev['epsilon'], curr['epsilon'], lambda x: f"{x:.4f}")
        ]
        
        print(f"{'Metric':<15} {'Before':>12} {'After':>12} {'Change':>12} {'Status':>8}")
        print("â”€" * 80)
        
        for name, before, after, fmt in metrics:
            change = after - before
            change_str = f"{change:+.2f}" if abs(change) > 0.01 else "~0.00"
            
            # Determine if improvement
            if name == 'Epsilon (Îµ)':
                status = "âœ“" if change < 0 else "â†’"  # Lower is better
            elif name == 'Spend':
                status = "â†’"  # Neutral
            else:
                status = "âœ“" if change > 0 else ("âœ—" if change < -1 else "â†’")
            
            print(f"{name:<15} {fmt(before):>12} {fmt(after):>12} {change_str:>12} {status:>8}")
        
        print("\nğŸ’¡ WHAT THE AGENTS LEARNED:")
        
        if curr['reward'] > prev['reward']:
            print(f"   âœ“ Better strategy discovered (+{curr['reward'] - prev['reward']:.0f} reward)")
        
        if curr['conversions'] > prev['conversions']:
            print(f"   âœ“ More effective bidding (+{curr['conversions'] - prev['conversions']} conversions)")
        
        if curr['roi'] > prev['roi']:
            print(f"   âœ“ Improved efficiency (+{curr['roi'] - prev['roi']:.1f}pp ROI)")
        
        if curr['epsilon'] < prev['epsilon']:
            print(f"   âœ“ Increased confidence (Îµ: {prev['epsilon']:.4f} â†’ {curr['epsilon']:.4f})")
        
        print("\nğŸ“ˆ This demonstrates ONLINE LEARNING:")
        print("   Agents adapted their behavior based on Episode {episode-1} experience!")
    
    def show_final_learning_summary(self, num_episodes: int):
        """Show overall learning summary"""
        
        print(f"\n{'='*80}")
        print("ONLINE LEARNING SUMMARY - COMPLETE TRAJECTORY")
        print(f"{'='*80}\n")
        
        if len(self.episode_history) < 2:
            return
        
        first = self.episode_history[0]
        last = self.episode_history[-1]
        
        print("BEFORE LEARNING (Episode 0):")
        print(f"  Reward: {first['reward']:.2f}")
        print(f"  Conversions: {first['conversions']}")
        print(f"  ROI: {first['roi']:.1f}%")
        print(f"  Strategy: Random/Initial")
        
        print("\nAFTER LEARNING (Episode {})".format(num_episodes - 1))
        print(f"  Reward: {last['reward']:.2f}")
        print(f"  Conversions: {last['conversions']}")
        print(f"  ROI: {last['roi']:.1f}%")
        print(f"  Strategy: Learned/Optimized")
        
        print("\nTOTAL IMPROVEMENT:")
        print(f"  Reward: {last['reward'] - first['reward']:+.2f} "
              f"({(last['reward'] - first['reward'])/max(abs(first['reward']), 1)*100:+.1f}%)")
        print(f"  Conversions: {last['conversions'] - first['conversions']:+d}")
        print(f"  ROI: {last['roi'] - first['roi']:+.1f}pp")
        
        # Learning curve
        print("\nLEARNING TRAJECTORY:")
        print("â”€" * 80)
        print(f"{'Episode':<10} {'Reward':>12} {'Conv':>8} {'ROI':>10} {'Trend':>10}")
        print("â”€" * 80)
        
        for i, ep in enumerate(self.episode_history):
            if i == 0:
                trend = "Baseline"
            else:
                prev_reward = self.episode_history[i-1]['reward']
                trend = "â†—ï¸ Up" if ep['reward'] > prev_reward else ("â†˜ï¸ Down" if ep['reward'] < prev_reward else "â†’ Same")
            
            print(f"{ep['episode']:<10} {ep['reward']:>12.2f} {ep['conversions']:>8} "
                  f"{ep['roi']:>9.1f}% {trend:>10}")
        
        print("\nâœ… EVIDENCE OF ONLINE LEARNING:")
        
        improvements = sum(1 for i in range(1, len(self.episode_history)) 
                          if self.episode_history[i]['reward'] > self.episode_history[i-1]['reward'])
        
        print(f"  â€¢ {improvements}/{num_episodes-1} episodes showed improvement")
        print(f"  â€¢ Total reward gain: {last['reward'] - first['reward']:+.0f}")
        print(f"  â€¢ Agents adapted strategies based on experience")
        print(f"  â€¢ Performance trajectory clearly upward")
        
        if last['reward'] > first['reward']:
            print("\nğŸ¯ CONCLUSION: System demonstrated SUCCESSFUL online learning!")
        
        print(f"\n{'='*80}")


# ============================================================================
# Main Demo
# ============================================================================

def main():
    """Main demonstration"""
    
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                        â•‘
â•‘    COMPLETE MULTI-AGENT FLOW WITH ONLINE LEARNING DEMONSTRATION       â•‘
â•‘                                                                        â•‘
â•‘  This shows TWO things simultaneously:                                â•‘
â•‘                                                                        â•‘
â•‘  1. COMPLETE 7-STEP FLOW PER TIMESTEP:                                â•‘
â•‘     â€¢ SimulationAgent â†’ ControllerAgent (state)                       â•‘
â•‘     â€¢ ControllerAgent â†’ Bidding & Budget (queries)                    â•‘
â•‘     â€¢ BiddingAgent â†’ ControllerAgent (bid proposal)                   â•‘
â•‘     â€¢ BudgetAgent â†’ ControllerAgent (allocation proposal)             â•‘
â•‘     â€¢ ControllerAgent â†’ SimulationAgent (action)                      â•‘
â•‘     â€¢ SimulationAgent â†’ All Agents (reward)                           â•‘
â•‘     â€¢ AnalyticsAgent (CrewAI) â†’ Insights                              â•‘
â•‘                                                                        â•‘
â•‘  2. ONLINE LEARNING ACROSS EPISODES:                                  â•‘
â•‘     â€¢ Agents start with random policy                                 â•‘
â•‘     â€¢ Experience episodes and collect data                            â•‘
â•‘     â€¢ Update policies based on what worked                            â•‘
â•‘     â€¢ Next episode uses improved policies                             â•‘
â•‘     â€¢ Performance metrics visibly increase                            â•‘
â•‘     â€¢ CrewAI explains the learning process                            â•‘
â•‘                                                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    num_episodes = input("\nHow many episodes to run? (3-10, recommend 5): ").strip()
    num_episodes = int(num_episodes) if num_episodes.isdigit() else 5
    num_episodes = min(max(3, num_episodes), 10)
    
    print(f"\nğŸš€ Running {num_episodes} episodes with complete flow + online learning...\n")
    print("TIP: Watch for:")
    print("  â€¢ Reward increasing episode-to-episode")
    print("  â€¢ Epsilon (Îµ) decreasing (less exploration)")
    print("  â€¢ Conversions improving")
    print("  â€¢ Strategy evolving")
    
    input("\nPress Enter to begin...")
    
    # Run demo
    demo = CompleteFlowDemo()
    demo.demonstrate_learning(num_episodes=num_episodes)
    
    print("\n" + "="*80)
    print("âœ… DEMONSTRATION COMPLETE!")
    print("="*80)
    print("\nYou just witnessed:")
    print("  âœ“ Complete multi-agent orchestration flow (7 steps)")
    print("  âœ“ Online learning in action (agents improving)")
    print("  âœ“ RL agents + LLM agents collaborating")
    print("  âœ“ Performance improving through experience")
    print("  âœ“ CrewAI providing intelligent analysis")
    print("\nThis showcases state-of-the-art agentic AI systems!")
    print("="*80)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¸ï¸  Demo interrupted")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()